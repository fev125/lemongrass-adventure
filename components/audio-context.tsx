"use client"

import { createContext, useContext, useCallback, useEffect, useRef, useState, type ReactNode } from "react"

interface AudioContextType {
  playClick: () => void
  playSuccess: () => void
  playError: () => void
  speak: (text: string) => void
  stopSpeaking: () => void
  isSpeaking: boolean
  ttsAvailable: boolean
  soundEnabled: boolean
  playBgm: () => void
  stopBgm: () => void
  isBgmPlaying: boolean
}

const AudioCtx = createContext<AudioContextType | null>(null)

export function useAudio() {
  const ctx = useContext(AudioCtx)
  if (!ctx) throw new Error("useAudio must be used within AudioProvider")
  return ctx
}

interface AudioProviderProps {
  children: ReactNode
  soundEnabled: boolean
}

export function AudioProvider({ children, soundEnabled }: AudioProviderProps) {
  const audioContextRef = useRef<AudioContext | null>(null)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [ttsAvailable, setTtsAvailable] = useState(false)
  const preferredVoiceRef = useRef<SpeechSynthesisVoice | null>(null)
  const [isBgmPlaying, setIsBgmPlaying] = useState(false)
  const bgmIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const bgmOscillatorsRef = useRef<OscillatorNode[]>([])
  const bgmGainRef = useRef<GainNode | null>(null)
  const bgmShouldPlayRef = useRef(false)

  useEffect(() => {
    const initAudio = () => {
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)()
      }
    }

    const checkTTS = () => {
      if (typeof window !== "undefined" && "speechSynthesis" in window) {
        const loadVoices = () => {
          const voices = speechSynthesis.getVoices()

          // æ—¥å¿—ï¼šè¾“å‡ºæ‰€æœ‰å¯ç”¨ä¸­æ–‡è¯­éŸ³
          const chineseVoices = voices.filter(
            (v) => v.lang.startsWith("zh-CN") || v.lang.startsWith("zh_CN") || v.lang === "zh",
          )
          console.log("[TTS] æ‰€æœ‰å¯ç”¨ä¸­æ–‡è¯­éŸ³:", chineseVoices.map((v) => ({
            name: v.name,
            lang: v.lang,
            voiceURI: v.voiceURI,
            localService: (v as any).localService,
            default: (v as any).default,
          })))

          // æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾é«˜è´¨é‡ä¸­æ–‡è¯­éŸ³
          // ä¼˜å…ˆçº§ï¼šiOS é«˜è´¨é‡è¯­éŸ³ > Chrome Google è¯­éŸ³ > å¥³å£° > å…¶ä»–æœ¬åœ°è¯­éŸ³
          const preferredVoicePatterns = [
            /tingting/i, // iOS é«˜è´¨é‡å¥³å£°
            /ting-ting/i, // macOS
            /siyi/i, // iOS é«˜è´¨é‡å¥³å£°
            /google.*æ™®é€šè¯/i, // Chrome é«˜è´¨é‡è¯­éŸ³
            /google/i, // Chrome å…¶ä»– Google è¯­éŸ³
            /microsoft.*xiaoxiao/i, // Edge å¥³å£°
            /microsoft.*yunxi/i, // Edge ç”·å£°
          ]

          let foundVoice: SpeechSynthesisVoice | null = null
          let foundReason = ""

          // å…ˆæ‰¾ä¼˜å…ˆè¯­éŸ³ï¼ˆiOS/Google ä¼˜å…ˆï¼‰ï¼Œæ”¯æŒæ­£åˆ™åŒ¹é…
          for (const pattern of preferredVoicePatterns) {
            const voice = chineseVoices.find((v) => pattern.test(v.name))
            if (voice) {
              foundVoice = voice
              foundReason = `âœ… æ‰¾åˆ°ä¼˜å…ˆè¯­éŸ³: ${voice.name} (åŒ¹é…: ${pattern})`
              break
            }
          }

          // å¦‚æœæ²¡æ‰¾åˆ°ä¼˜å…ˆè¯­éŸ³ï¼Œå°è¯•é€‰æ‹©å¥³å£°ï¼ˆé€šå¸¸æ›´è‡ªç„¶ï¼Œé€‚åˆå„¿ç«¥åº”ç”¨ï¼‰
          if (!foundVoice) {
            // Chrome ä¸­å¥³å£°é€šå¸¸åŒ…å«ï¼šå¥³ã€femaleã€woman ç­‰å…³é”®è¯ï¼Œæˆ–æ’é™¤æ˜æ˜¾çš„ç”·å£°å…³é”®è¯
            const femaleKeywords = [/å¥³/i, /female/i, /woman/i, /xiaoxiao/i]
            const maleKeywords = [/ç”·/i, /male/i, /man/i, /eddy/i, /yunxi/i]
            
            const femaleVoices = chineseVoices.filter((v) => {
              const name = v.name.toLowerCase()
              const hasFemale = femaleKeywords.some((k) => k.test(name))
              const hasMale = maleKeywords.some((k) => k.test(name))
              return hasFemale && !hasMale
            })
            
            if (femaleVoices.length > 0) {
              const localFemale = femaleVoices.filter((v) => (v as any).localService !== false)
              foundVoice = localFemale[0] || femaleVoices[0]
              foundReason = `âœ… æ‰¾åˆ°å¥³å£°è¯­éŸ³: ${foundVoice.name} (æ›´é€‚åˆå„¿ç«¥åº”ç”¨)`
            }
          }

          // å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œä¼˜å…ˆé€‰æ‹© localService: true çš„è¯­éŸ³ï¼ˆæœ¬åœ°é«˜è´¨é‡è¯­éŸ³ï¼‰
          if (!foundVoice) {
            const localVoices = chineseVoices.filter((v) => (v as any).localService !== false)
            if (localVoices.length > 0) {
              // ä¼˜å…ˆé€‰æ‹©éé»˜è®¤è¯­éŸ³ï¼ˆé€šå¸¸è´¨é‡æ›´å¥½ï¼‰
              foundVoice = localVoices.find((v) => !(v as any).default) || localVoices[0]
              foundReason = `âš ï¸ æœªæ‰¾åˆ°ä¼˜å…ˆè¯­éŸ³ï¼Œä½¿ç”¨æœ¬åœ°è¯­éŸ³: ${foundVoice.name}`
            } else {
              // æœ€åé€‰æ‹©ä»»ä½•ä¸­æ–‡è¯­éŸ³
              foundVoice = chineseVoices[0] || null
              if (foundVoice) {
                foundReason = `âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°è¯­éŸ³ï¼Œä½¿ç”¨é»˜è®¤ä¸­æ–‡è¯­éŸ³: ${foundVoice.name}`
              } else {
                foundReason = `âŒ æœªæ‰¾åˆ°ä»»ä½•ä¸­æ–‡è¯­éŸ³`
              }
            }
          }

          if (foundVoice) {
            preferredVoiceRef.current = foundVoice
            setTtsAvailable(true)
            console.log(`[TTS] å·²é€‰æ‹©è¯­éŸ³:`, {
              name: foundVoice.name,
              lang: foundVoice.lang,
              voiceURI: foundVoice.voiceURI,
              localService: (foundVoice as any).localService,
              reason: foundReason,
            })
            
            // æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¥½çš„è¯­éŸ³å¯ç”¨
            const betterVoices = chineseVoices.filter((v) => {
              const isPreferred = preferredVoicePatterns.some((pattern) => pattern.test(v.name))
              const isLocal = (v as any).localService !== false
              return isPreferred && isLocal && v.name !== foundVoice!.name
            })
            
            if (betterVoices.length > 0) {
              console.warn(`[TTS] âš ï¸ æ£€æµ‹åˆ°æ›´å¥½çš„è¯­éŸ³ä½†æœªä½¿ç”¨:`, betterVoices.map((v) => ({
                name: v.name,
                lang: v.lang,
                voiceURI: v.voiceURI,
                localService: (v as any).localService,
              })))
              console.warn(`[TTS] å½“å‰é€‰æ‹©åŸå› : ${foundReason}`)
            }
            
            // æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨è¯­éŸ³çš„å¯¹æ¯”ï¼Œå¸®åŠ©ç†è§£é€‰æ‹©
            const allLocalVoices = chineseVoices.filter((v) => (v as any).localService !== false)
            if (allLocalVoices.length > 1) {
              console.log(`[TTS] ğŸ“Š æ‰€æœ‰æœ¬åœ°ä¸­æ–‡è¯­éŸ³å¯¹æ¯” (${allLocalVoices.length}ä¸ª):`, allLocalVoices.map((v) => {
                const isPreferred = preferredVoicePatterns.some((pattern) => pattern.test(v.name))
                const isFemale = /å¥³|female|woman|xiaoxiao/i.test(v.name) && !/ç”·|male|man|eddy|yunxi/i.test(v.name)
                return {
                  name: v.name,
                  lang: v.lang,
                  isSelected: v.name === foundVoice!.name,
                  isPreferred: isPreferred ? "âœ…" : "âŒ",
                  isFemale: isFemale ? "ğŸ‘©" : "ğŸ‘¨",
                  localService: (v as any).localService,
                }
              }))
              console.log(`[TTS] ğŸ’¡ é€‰æ‹©é€»è¾‘: ä¼˜å…ˆè¯­éŸ³ > å¥³å£° > æœ¬åœ°è¯­éŸ³ > é»˜è®¤è¯­éŸ³`)
            }
          } else {
            console.error("[TTS] âŒ æœªæ‰¾åˆ°å¯ç”¨ä¸­æ–‡è¯­éŸ³")
          }
        }

        if (speechSynthesis.getVoices().length > 0) {
          loadVoices()
        }
        speechSynthesis.onvoiceschanged = loadVoices

        // è®¾ç½®è¶…æ—¶ï¼Œå¦‚æœ3ç§’å†…æ²¡æœ‰åŠ è½½åˆ°è¯­éŸ³ï¼Œä¹Ÿæ ‡è®°ä¸ºå¯ç”¨ï¼ˆä½¿ç”¨é»˜è®¤è¯­éŸ³ï¼‰
        setTimeout(() => {
          if (!preferredVoiceRef.current && speechSynthesis.getVoices().length > 0) {
            // å°è¯•ä½¿ç”¨é»˜è®¤ä¸­æ–‡è¯­éŸ³
            const defaultVoice = speechSynthesis.getVoices().find(
              (v) => v.lang.startsWith("zh-CN") || v.lang.startsWith("zh_CN") || v.lang === "zh",
            )
            if (defaultVoice) {
              preferredVoiceRef.current = defaultVoice
              setTtsAvailable(true)
              console.log("[TTS] â° è¶…æ—¶åä½¿ç”¨é»˜è®¤ä¸­æ–‡è¯­éŸ³:", {
                name: defaultVoice.name,
                lang: defaultVoice.lang,
                voiceURI: defaultVoice.voiceURI,
                localService: (defaultVoice as any).localService,
              })
            } else {
              console.warn("[TTS] â° è¶…æ—¶åä»æœªæ‰¾åˆ°ä¸­æ–‡è¯­éŸ³")
            }
          }
        }, 3000)
      }
    }

    initAudio()
    checkTTS()

    return () => {
      if (audioContextRef.current) {
        audioContextRef.current.close()
      }
    }
  }, [])

  const playClick = useCallback(() => {
    if (!soundEnabled || !audioContextRef.current) return

    const ctx = audioContextRef.current
    if (ctx.state === "suspended") ctx.resume()

    const oscillator = ctx.createOscillator()
    const gainNode = ctx.createGain()

    oscillator.type = "sine"
    oscillator.frequency.setValueAtTime(800, ctx.currentTime)
    oscillator.frequency.exponentialRampToValueAtTime(600, ctx.currentTime + 0.08)
    gainNode.gain.setValueAtTime(0.25, ctx.currentTime)
    gainNode.gain.exponentialRampToValueAtTime(0.01, ctx.currentTime + 0.08)

    oscillator.connect(gainNode)
    gainNode.connect(ctx.destination)

    oscillator.start(ctx.currentTime)
    oscillator.stop(ctx.currentTime + 0.08)
  }, [soundEnabled])

  const playSuccess = useCallback(() => {
    if (!soundEnabled || !audioContextRef.current) return

    const ctx = audioContextRef.current
    if (ctx.state === "suspended") ctx.resume()

    const frequencies = [523.25, 659.25, 783.99, 1046.5]

    frequencies.forEach((freq, i) => {
      const oscillator = ctx.createOscillator()
      const gainNode = ctx.createGain()

      oscillator.type = "triangle"
      oscillator.frequency.setValueAtTime(freq, ctx.currentTime)
      gainNode.gain.setValueAtTime(0.2, ctx.currentTime + i * 0.12)
      gainNode.gain.exponentialRampToValueAtTime(0.01, ctx.currentTime + 0.5 + i * 0.12)

      oscillator.connect(gainNode)
      gainNode.connect(ctx.destination)

      oscillator.start(ctx.currentTime + i * 0.12)
      oscillator.stop(ctx.currentTime + 0.6 + i * 0.12)
    })
  }, [soundEnabled])

  const playError = useCallback(() => {
    if (!soundEnabled || !audioContextRef.current) return

    const ctx = audioContextRef.current
    if (ctx.state === "suspended") ctx.resume()

    const oscillator = ctx.createOscillator()
    const gainNode = ctx.createGain()

    oscillator.type = "sine"
    oscillator.frequency.setValueAtTime(300, ctx.currentTime)
    oscillator.frequency.exponentialRampToValueAtTime(200, ctx.currentTime + 0.2)
    gainNode.gain.setValueAtTime(0.15, ctx.currentTime)
    gainNode.gain.exponentialRampToValueAtTime(0.01, ctx.currentTime + 0.2)

    oscillator.connect(gainNode)
    gainNode.connect(ctx.destination)

    oscillator.start(ctx.currentTime)
    oscillator.stop(ctx.currentTime + 0.2)
  }, [soundEnabled])

  const speak = useCallback(
    (text: string) => {
      if (!soundEnabled || !ttsAvailable) return

      // åœæ­¢å½“å‰æ’­æ”¾
      speechSynthesis.cancel()

      // ä½¿ç”¨æµè§ˆå™¨æœ¬åœ° TTSï¼ˆä¼˜å…ˆ iOS é«˜è´¨é‡è¯­éŸ³ï¼‰
      const utterance = new SpeechSynthesisUtterance(text)
      
      if (preferredVoiceRef.current) {
        utterance.voice = preferredVoiceRef.current
        console.log(`[TTS] ğŸ¤ æ­£åœ¨ä½¿ç”¨è¯­éŸ³:`, {
          name: preferredVoiceRef.current.name,
          lang: preferredVoiceRef.current.lang,
          voiceURI: preferredVoiceRef.current.voiceURI,
          localService: (preferredVoiceRef.current as any).localService,
          text: text.substring(0, 50) + (text.length > 50 ? "..." : ""),
        })
      } else {
        console.warn("[TTS] âš ï¸ æœªè®¾ç½®è¯­éŸ³ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿé»˜è®¤è¯­éŸ³")
      }
      
      // è®¾ç½®è¯­éŸ³å‚æ•°ï¼Œä¼˜åŒ–æ™®é€šè¯æ•ˆæœ
      utterance.lang = "zh-CN"
      utterance.rate = 0.9 // è¯­é€Ÿï¼ˆiOS æ¨èå€¼ï¼‰
      utterance.pitch = 1.1 // éŸ³è°ƒï¼ˆç•¥é«˜æ›´è‡ªç„¶ï¼‰
      utterance.volume = 1

      // çŠ¶æ€ç®¡ç†
      utterance.onstart = () => setIsSpeaking(true)
      utterance.onend = () => setIsSpeaking(false)
      utterance.onerror = (e: SpeechSynthesisErrorEvent) => {
        // "canceled" and "interrupted" are expected when we call speechSynthesis.cancel()
        if (e.error === "canceled" || e.error === "interrupted") {
          console.log("[TTS] â¹ï¸ è¯­éŸ³å·²åœæ­¢:", e.error)
        } else {
          console.error("[TTS] âŒ è¯­éŸ³æ’­æ”¾é”™è¯¯:", {
            error: e.error,
            charIndex: e.charIndex,
            elapsedTime: e.elapsedTime,
            name: e.name,
          })
        }
        setIsSpeaking(false)
      }

      speechSynthesis.speak(utterance)
    },
    [soundEnabled, ttsAvailable],
  )

  const stopSpeaking = useCallback(() => {
    speechSynthesis.cancel()
    setIsSpeaking(false)
  }, [])

  // æ¬¢å¿«çš„èƒŒæ™¯éŸ³ä¹ - ä½¿ç”¨ Web Audio API ç”Ÿæˆç®€å•çš„å¾ªç¯æ—‹å¾‹
  const playBgm = useCallback(() => {
    if (!soundEnabled || !audioContextRef.current || bgmShouldPlayRef.current) return

    const ctx = audioContextRef.current
    if (ctx.state === "suspended") ctx.resume()

    bgmShouldPlayRef.current = true

    // åˆ›å»ºä¸»éŸ³é‡èŠ‚ç‚¹
    const masterGain = ctx.createGain()
    masterGain.gain.setValueAtTime(0.12, ctx.currentTime)
    masterGain.connect(ctx.destination)
    bgmGainRef.current = masterGain

    // æ¬¢å¿«çš„å„¿ç«¥æ­Œæ›²æ—‹å¾‹éŸ³ç¬¦ (ç®€åŒ–ç‰ˆå°æ˜Ÿæ˜Ÿ + æ¬¢å¿«èŠ‚å¥)
    const melody = [
      // Cå¤§è°ƒæ¬¢å¿«æ—‹å¾‹
      { note: 523.25, duration: 0.3 }, // C5
      { note: 523.25, duration: 0.3 }, // C5
      { note: 783.99, duration: 0.3 }, // G5
      { note: 783.99, duration: 0.3 }, // G5
      { note: 880.00, duration: 0.3 }, // A5
      { note: 880.00, duration: 0.3 }, // A5
      { note: 783.99, duration: 0.6 },  // G5 (é•¿éŸ³)
      { note: 698.46, duration: 0.3 }, // F5
      { note: 698.46, duration: 0.3 }, // F5
      { note: 659.25, duration: 0.3 }, // E5
      { note: 659.25, duration: 0.3 }, // E5
      { note: 587.33, duration: 0.3 }, // D5
      { note: 587.33, duration: 0.3 }, // D5
      { note: 523.25, duration: 0.6 },  // C5 (é•¿éŸ³)
      // ä¼‘æ­¢ç¬¦ (é™éŸ³é—´éš”)
      { note: 0, duration: 0.5 },
    ]

    let noteIndex = 0

    const playNote = () => {
      if (!bgmShouldPlayRef.current || !bgmGainRef.current || !audioContextRef.current) return

      const currentCtx = audioContextRef.current
      const note = melody[noteIndex]

      // å¦‚æœä¸æ˜¯ä¼‘æ­¢ç¬¦ï¼Œæ’­æ”¾éŸ³ç¬¦
      if (note.note > 0) {
        const osc = currentCtx.createOscillator()
        const noteGain = currentCtx.createGain()

        // ä½¿ç”¨ä¸‰è§’æ³¢ï¼Œæ›´æŸ”å’Œé€‚åˆå„¿ç«¥
        osc.type = "triangle"
        osc.frequency.setValueAtTime(note.note, currentCtx.currentTime)

        // éŸ³ç¬¦åŒ…ç»œ
        noteGain.gain.setValueAtTime(0, currentCtx.currentTime)
        noteGain.gain.linearRampToValueAtTime(0.25, currentCtx.currentTime + 0.03)
        noteGain.gain.linearRampToValueAtTime(0.15, currentCtx.currentTime + note.duration * 0.5)
        noteGain.gain.linearRampToValueAtTime(0, currentCtx.currentTime + note.duration * 0.9)

        osc.connect(noteGain)
        noteGain.connect(bgmGainRef.current)

        osc.start(currentCtx.currentTime)
        osc.stop(currentCtx.currentTime + note.duration)

        bgmOscillatorsRef.current.push(osc)

        // æ¸…ç†å·²ç»“æŸçš„æŒ¯è¡å™¨
        osc.onended = () => {
          const idx = bgmOscillatorsRef.current.indexOf(osc)
          if (idx > -1) bgmOscillatorsRef.current.splice(idx, 1)
        }
      }

      noteIndex = (noteIndex + 1) % melody.length
    }

    // æ’­æ”¾ç¬¬ä¸€ä¸ªéŸ³ç¬¦
    playNote()

    // è®¾ç½®å¾ªç¯æ’­æ”¾
    const scheduleNext = () => {
      if (!bgmShouldPlayRef.current) return

      const prevIndex = noteIndex === 0 ? melody.length - 1 : noteIndex - 1
      const currentDelay = melody[prevIndex].duration * 1000

      bgmIntervalRef.current = setTimeout(() => {
        if (bgmShouldPlayRef.current) {
          playNote()
          scheduleNext()
        }
      }, currentDelay)
    }

    scheduleNext()
    setIsBgmPlaying(true)
  }, [soundEnabled])

  const stopBgm = useCallback(() => {
    // è®¾ç½®åœæ­¢æ ‡å¿—
    bgmShouldPlayRef.current = false

    // åœæ­¢æ‰€æœ‰æŒ¯è¡å™¨
    bgmOscillatorsRef.current.forEach((osc) => {
      try {
        osc.stop()
      } catch {
        // å·²ç»åœæ­¢äº†
      }
    })
    bgmOscillatorsRef.current = []

    // æ¸…é™¤å®šæ—¶å™¨
    if (bgmIntervalRef.current) {
      clearTimeout(bgmIntervalRef.current)
      bgmIntervalRef.current = null
    }

    // æ–­å¼€éŸ³é‡èŠ‚ç‚¹
    if (bgmGainRef.current) {
      bgmGainRef.current.disconnect()
      bgmGainRef.current = null
    }

    setIsBgmPlaying(false)
  }, [])

  return (
    <AudioCtx.Provider
      value={{
        playClick,
        playSuccess,
        playError,
        speak,
        stopSpeaking,
        isSpeaking,
        ttsAvailable,
        soundEnabled,
        playBgm,
        stopBgm,
        isBgmPlaying,
      }}
    >
      {children}
    </AudioCtx.Provider>
  )
}
